{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eumnH0bBqXee"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "  def __init__(self,input_size, learning_rate=0.001,epooch=1000):\n",
        "    self.weight=np.zeros(input_size+1)\n",
        "    self.learning_rate=learning_rate\n",
        "    self.epooch=epooch\n",
        "\n",
        "  def activation(self,x):\n",
        "    return 1 if x>0  else 0\n",
        "\n",
        "  def predict(self,x):\n",
        "    z=self.weight.T.dot(x)\n",
        "    return self.activation(z)\n",
        "\n",
        "  def training(self,training_input,labels):\n",
        "    for _ in range(self.epooch):\n",
        "      for x,y in zip(training_input,labels):\n",
        "        x=np.insert(x,0,1)\n",
        "        prediction=self.predict(x)\n",
        "        self.weight +=self.learning_rate*(y-prediction) *x\n",
        "\n",
        "  def accuracy(self,test_input,test_labels):\n",
        "    correctness=0\n",
        "    for x,y in zip(test_input,test_labels):\n",
        "      x = np.insert(x, 0, 1)\n",
        "      y_predict=self.predict(x)\n",
        "      if y_predict==y:\n",
        "        correctness+=1\n",
        "\n",
        "    return correctness/len(test_input)\n",
        "\n"
      ],
      "metadata": {
        "id": "zm4MNuTOqyHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_input=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "lables=np.array([0,0,0,1])\n",
        "\n",
        "perceptron= Perceptron(input_size=2)\n",
        "perceptron.training(training_input,lables)\n"
      ],
      "metadata": {
        "id": "Xb2lPCdstWNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX4y0JgWuIbL",
        "outputId": "25332f90-7747-4bea-9f2e-f250667b46c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.002,  0.002,  0.001])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "test_labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    test_input_with_bias = np.insert(test_input, 0, 1)  # Adding bias term\n",
        "    print(f\"Input: {test_input}, Predicted Output: {perceptron.predict(test_input_with_bias)}\")\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = perceptron.accuracy(test_inputs, test_labels)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AYr_6afwReP",
        "outputId": "d0481832-9a82-4080-b4d3-ad03952cd58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 0\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 1\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for test_input in test_inputs:\n",
        "    test_input_with_bias = np.insert(test_input, 0, 1)  # Adding bias term\n",
        "    print(f\"Input: {test_input}, Predicted Output: {perceptron.predict(test_input_with_bias)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHeu8zaIuslK",
        "outputId": "d331c719-5e0f-4977-d552-c8339016dd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 0\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# There is limitation in perceptrons - like it doesn't work with XOR case."
      ],
      "metadata": {
        "id": "SWz15aQyxTXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_input=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "lables=np.array([0,1,1,0])\n",
        "\n",
        "perceptron2= Perceptron(input_size=2)\n",
        "perceptron2.training(training_input,lables)"
      ],
      "metadata": {
        "id": "F2tPzrIWw3Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron2.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfQmrmjTxE3H",
        "outputId": "af89b2fc-5bb0-4629-cfa7-2deb272d25fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.001, -0.001,  0.   ])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for test_input in test_inputs:\n",
        "    test_input_with_bias = np.insert(test_input, 0, 1)  # Adding bias term\n",
        "    print(f\"Input: {test_input}, Predicted Output: {perceptron.predict(test_input_with_bias)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28fDgW1sxJGZ",
        "outputId": "87958dc9-ac56-4d8a-e933-ac91e82eb142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 0\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1lOzmexxPTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kN8YWHW6vF7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In raw Python"
      ],
      "metadata": {
        "id": "2HZOjli5vHTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "  def __init__(self,input_size, learning_rate=0.001,epooch=1000):\n",
        "    self.weight=np.zeros(input_size+1)\n",
        "    self.learning_rate=learning_rate\n",
        "    self.epooch=epooch\n",
        "\n",
        "  def activation(self,x):\n",
        "    return 1 if x>0  else 0\n",
        "\n",
        "  def predict(self,x):\n",
        "    z=self.weight.T.dot(x)\n",
        "    return self.activation(z)\n",
        "\n",
        "  def training(self,training_input,labels):\n",
        "    for _ in range(self.epooch):\n",
        "      for x,y in zip(training_input,labels):\n",
        "        x=np.insert(x,0,1)\n",
        "        prediction=self.predict(x)\n",
        "        self.weight +=self.learning_rate*(y-prediction) *x\n",
        "\n",
        "  def accuracy(self,test_input,test_labels):\n",
        "    correctness=0\n",
        "    for x,y in zip(test_input,test_labels):\n",
        "      x = np.insert(x, 0, 1)\n",
        "      y_predict=self.predict(x)\n",
        "      if y_predict==y:\n",
        "        correctness+=1\n",
        "\n",
        "    return correctness/len(test_input)\n",
        "\n",
        "training_input=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "lables=np.array([0,0,0,1])\n",
        "\n",
        "perceptron= Perceptron(input_size=2)\n",
        "perceptron.training(training_input,lables)\n",
        "\n",
        "test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "test_labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    test_input_with_bias = np.insert(test_input, 0, 1)  # Adding bias term\n",
        "    print(f\"Input: {test_input}, Predicted Output: {perceptron.predict(test_input_with_bias)}\")\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = perceptron.accuracy(test_inputs, test_labels)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2q2WT8jvF_D",
        "outputId": "acd163bc-688e-499a-c062-1bbe40d05c5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Predicted Output: 0\n",
            "Input: [0 1], Predicted Output: 0\n",
            "Input: [1 0], Predicted Output: 0\n",
            "Input: [1 1], Predicted Output: 1\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vxR6D79EvKNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DdTS5aRvKvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In tensorflow -\n"
      ],
      "metadata": {
        "id": "fm560-JuvLPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Sample data\n",
        "X = tf.constant([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=tf.float32)\n",
        "y = tf.constant([[0.0], [1.0], [1.0], [1.0]], dtype=tf.float32)\n",
        "\n",
        "# Define the Perceptron model\n",
        "model=Sequential([Dense(1,input_dim=2,activation='sigmoid')])\n",
        "\n",
        "#compile model\n",
        "model.compile(optimizer='sgd',loss='binary_crossentropy')\n",
        "\n",
        "epochs=1000\n",
        "\n",
        "history=model.fit(X,y,epochs=epochs,verbose=0)\n",
        "\n",
        "# Print the final loss\n",
        "final_loss = history.history['loss'][-1]\n",
        "print(f'Final Loss: {final_loss:.4f}')\n",
        "\n",
        "# Testing the model\n",
        "predictions = model.predict(X).round()\n",
        "print(f'Predictions:\\n{predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kASU1ubvPMm",
        "outputId": "859af890-1b9c-4431-e8cd-395e4a84fdd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Loss: 0.2678\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "Predictions:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In pytorch"
      ],
      "metadata": {
        "id": "8R28l6MpvLVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "  def __init__(self,input_dim):\n",
        "    super(Perceptron,self).__init__()\n",
        "    self.fc= nn.Linear(input_dim,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.fc(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "# Sample data\n",
        "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=torch.float32)\n",
        "y = torch.tensor([[0.0], [1.0], [1.0], [1.0]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "input_dim= X.shape[1]\n",
        "model=Perceptron(input_dim)\n",
        "criterion=nn.BCELoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.001)\n",
        "\n",
        "\n",
        "epochs= 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  outputs=model(X)\n",
        "  loss= criterion(outputs,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_output=model(X)\n",
        "  predictions=test_output.round()\n",
        "  print(f'Predictions:\\n{predictions}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tG5ybL-vbGv",
        "outputId": "b763c18c-7285-4a25-fae1-b649f76cc924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/10000], Loss: 0.9427\n",
            "Epoch [200/10000], Loss: 0.9112\n",
            "Epoch [300/10000], Loss: 0.8818\n",
            "Epoch [400/10000], Loss: 0.8544\n",
            "Epoch [500/10000], Loss: 0.8289\n",
            "Epoch [600/10000], Loss: 0.8052\n",
            "Epoch [700/10000], Loss: 0.7831\n",
            "Epoch [800/10000], Loss: 0.7625\n",
            "Epoch [900/10000], Loss: 0.7435\n",
            "Epoch [1000/10000], Loss: 0.7257\n",
            "Epoch [1100/10000], Loss: 0.7092\n",
            "Epoch [1200/10000], Loss: 0.6938\n",
            "Epoch [1300/10000], Loss: 0.6795\n",
            "Epoch [1400/10000], Loss: 0.6661\n",
            "Epoch [1500/10000], Loss: 0.6537\n",
            "Epoch [1600/10000], Loss: 0.6421\n",
            "Epoch [1700/10000], Loss: 0.6312\n",
            "Epoch [1800/10000], Loss: 0.6210\n",
            "Epoch [1900/10000], Loss: 0.6115\n",
            "Epoch [2000/10000], Loss: 0.6026\n",
            "Epoch [2100/10000], Loss: 0.5942\n",
            "Epoch [2200/10000], Loss: 0.5863\n",
            "Epoch [2300/10000], Loss: 0.5789\n",
            "Epoch [2400/10000], Loss: 0.5719\n",
            "Epoch [2500/10000], Loss: 0.5652\n",
            "Epoch [2600/10000], Loss: 0.5590\n",
            "Epoch [2700/10000], Loss: 0.5531\n",
            "Epoch [2800/10000], Loss: 0.5474\n",
            "Epoch [2900/10000], Loss: 0.5421\n",
            "Epoch [3000/10000], Loss: 0.5370\n",
            "Epoch [3100/10000], Loss: 0.5322\n",
            "Epoch [3200/10000], Loss: 0.5276\n",
            "Epoch [3300/10000], Loss: 0.5232\n",
            "Epoch [3400/10000], Loss: 0.5190\n",
            "Epoch [3500/10000], Loss: 0.5150\n",
            "Epoch [3600/10000], Loss: 0.5111\n",
            "Epoch [3700/10000], Loss: 0.5074\n",
            "Epoch [3800/10000], Loss: 0.5039\n",
            "Epoch [3900/10000], Loss: 0.5005\n",
            "Epoch [4000/10000], Loss: 0.4972\n",
            "Epoch [4100/10000], Loss: 0.4940\n",
            "Epoch [4200/10000], Loss: 0.4909\n",
            "Epoch [4300/10000], Loss: 0.4880\n",
            "Epoch [4400/10000], Loss: 0.4851\n",
            "Epoch [4500/10000], Loss: 0.4823\n",
            "Epoch [4600/10000], Loss: 0.4796\n",
            "Epoch [4700/10000], Loss: 0.4770\n",
            "Epoch [4800/10000], Loss: 0.4745\n",
            "Epoch [4900/10000], Loss: 0.4720\n",
            "Epoch [5000/10000], Loss: 0.4696\n",
            "Epoch [5100/10000], Loss: 0.4672\n",
            "Epoch [5200/10000], Loss: 0.4649\n",
            "Epoch [5300/10000], Loss: 0.4627\n",
            "Epoch [5400/10000], Loss: 0.4605\n",
            "Epoch [5500/10000], Loss: 0.4584\n",
            "Epoch [5600/10000], Loss: 0.4563\n",
            "Epoch [5700/10000], Loss: 0.4543\n",
            "Epoch [5800/10000], Loss: 0.4523\n",
            "Epoch [5900/10000], Loss: 0.4503\n",
            "Epoch [6000/10000], Loss: 0.4484\n",
            "Epoch [6100/10000], Loss: 0.4465\n",
            "Epoch [6200/10000], Loss: 0.4447\n",
            "Epoch [6300/10000], Loss: 0.4429\n",
            "Epoch [6400/10000], Loss: 0.4411\n",
            "Epoch [6500/10000], Loss: 0.4393\n",
            "Epoch [6600/10000], Loss: 0.4376\n",
            "Epoch [6700/10000], Loss: 0.4359\n",
            "Epoch [6800/10000], Loss: 0.4342\n",
            "Epoch [6900/10000], Loss: 0.4326\n",
            "Epoch [7000/10000], Loss: 0.4309\n",
            "Epoch [7100/10000], Loss: 0.4293\n",
            "Epoch [7200/10000], Loss: 0.4277\n",
            "Epoch [7300/10000], Loss: 0.4262\n",
            "Epoch [7400/10000], Loss: 0.4246\n",
            "Epoch [7500/10000], Loss: 0.4231\n",
            "Epoch [7600/10000], Loss: 0.4216\n",
            "Epoch [7700/10000], Loss: 0.4201\n",
            "Epoch [7800/10000], Loss: 0.4186\n",
            "Epoch [7900/10000], Loss: 0.4172\n",
            "Epoch [8000/10000], Loss: 0.4157\n",
            "Epoch [8100/10000], Loss: 0.4143\n",
            "Epoch [8200/10000], Loss: 0.4129\n",
            "Epoch [8300/10000], Loss: 0.4115\n",
            "Epoch [8400/10000], Loss: 0.4101\n",
            "Epoch [8500/10000], Loss: 0.4087\n",
            "Epoch [8600/10000], Loss: 0.4074\n",
            "Epoch [8700/10000], Loss: 0.4060\n",
            "Epoch [8800/10000], Loss: 0.4047\n",
            "Epoch [8900/10000], Loss: 0.4034\n",
            "Epoch [9000/10000], Loss: 0.4021\n",
            "Epoch [9100/10000], Loss: 0.4007\n",
            "Epoch [9200/10000], Loss: 0.3995\n",
            "Epoch [9300/10000], Loss: 0.3982\n",
            "Epoch [9400/10000], Loss: 0.3969\n",
            "Epoch [9500/10000], Loss: 0.3957\n",
            "Epoch [9600/10000], Loss: 0.3944\n",
            "Epoch [9700/10000], Loss: 0.3932\n",
            "Epoch [9800/10000], Loss: 0.3919\n",
            "Epoch [9900/10000], Loss: 0.3907\n",
            "Epoch [10000/10000], Loss: 0.3895\n",
            "Predictions:\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    }
  ]
}